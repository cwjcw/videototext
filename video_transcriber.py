import os
import moviepy.editor as mp
from pydub import AudioSegment
from pydub.silence import split_on_silence
import whisper

def extract_audio_from_video(video_path, audio_output_path):
    """
    ä»è§†é¢‘æ–‡ä»¶ä¸­æå–éŸ³é¢‘ã€‚
    """
    try:
        video = mp.VideoFileClip(video_path)
        video.audio.write_audiofile(audio_output_path)
        print(f"éŸ³é¢‘å·²æˆåŠŸæå–åˆ°: {audio_output_path}")
        return True
    except Exception as e:
        print(f"æå–éŸ³é¢‘æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False

def transcribe_audio_with_whisper(audio_path, model_name="base"):
    """
    ä½¿ç”¨ OpenAI Whisper æ¨¡å‹å°†éŸ³é¢‘è½¬æ¢ä¸ºæ–‡æœ¬ã€‚
    model_name å¯ä»¥æ˜¯ "tiny", "base", "small", "medium", "large"ã€‚
    æ›´å¤§çš„æ¨¡å‹å‡†ç¡®åº¦æ›´é«˜ï¼Œä½†éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚
    """
    try:
        print(f"æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹: {model_name}...")
        model = whisper.load_model(model_name)
        print(f"æ­£åœ¨è½¬å½•éŸ³é¢‘: {audio_path}...")
        result = model.transcribe(audio_path)
        return result["text"]
    except Exception as e:
        print(f"è½¬å½•éŸ³é¢‘æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def split_and_transcribe_audio(audio_path, output_dir="audio_chunks", min_silence_len=500, silence_thresh=-40, model_name="base"):
    """
    å°†éŸ³é¢‘æ–‡ä»¶åˆ†å‰²æˆå°å—ï¼Œç„¶åé€å—è½¬å½•ã€‚
    è¿™å¯¹äºé•¿éŸ³é¢‘æ–‡ä»¶æˆ–å†…å­˜å—é™çš„æƒ…å†µéå¸¸æœ‰ç”¨ã€‚
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    try:
        audio = AudioSegment.from_file(audio_path)
        chunks = split_on_silence(audio,
                                  min_silence_len=min_silence_len,  # è¯†åˆ«ä¸ºé™éŸ³çš„æœ€å°é•¿åº¦ (æ¯«ç§’)
                                  silence_thresh=silence_thresh,    # ä½äºæ­¤åˆ†è´å€¼çš„è¢«è®¤ä¸ºæ˜¯é™éŸ³
                                  keep_silence=200                  # ä¿ç•™é™éŸ³çš„å‰å200æ¯«ç§’
                                 )

        full_text = []
        model = whisper.load_model(model_name)
        print(f"æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹: {model_name}...")

        for i, chunk in enumerate(chunks):
            chunk_filename = os.path.join(output_dir, f"chunk_{i}.wav")
            chunk.export(chunk_filename, format="wav")
            print(f"æ­£åœ¨è½¬å½•åˆ†å— {i+1}/{len(chunks)}: {chunk_filename}")
            
            # ä½¿ç”¨ Whisper è½¬å½•æ¯ä¸ªåˆ†å—
            result = model.transcribe(chunk_filename)
            text = result["text"]
            full_text.append(text)
            print(f"åˆ†å— {i+1} è½¬å½•å®Œæˆã€‚")
            
            # ç«‹å³åˆ é™¤å·²å¤„ç†çš„åˆ†å—æ–‡ä»¶ä»¥èŠ‚çœç©ºé—´
            try:
                os.remove(chunk_filename)
                print(f"âœ“ å·²åˆ é™¤åˆ†å—æ–‡ä»¶: {chunk_filename}")
            except Exception as e:
                print(f"âš ï¸  åˆ é™¤åˆ†å—æ–‡ä»¶æ—¶å‡ºé”™: {e}")

        return " ".join(full_text)
    except Exception as e:
        print(f"åˆ†æ®µå’Œè½¬å½•éŸ³é¢‘æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def get_filename_without_extension(file_path):
    """
    è·å–æ–‡ä»¶åï¼ˆä¸åŒ…å«æ‰©å±•åå’Œè·¯å¾„ï¼‰
    """
    base_name = os.path.basename(file_path)  # è·å–æ–‡ä»¶åï¼ˆåŒ…å«æ‰©å±•åï¼‰
    filename_without_ext = os.path.splitext(base_name)[0]  # å»é™¤æ‰©å±•å
    return filename_without_ext

def process_video_to_text(video_path, model_name="base", use_segmentation=False):
    """
    å¤„ç†è§†é¢‘æ–‡ä»¶ï¼Œæå–éŸ³é¢‘å¹¶è½¬å½•ä¸ºæ–‡æœ¬
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        model_name: Whisperæ¨¡å‹åç§° ("tiny", "base", "small", "medium", "large")
        use_segmentation: æ˜¯å¦ä½¿ç”¨åˆ†æ®µè½¬å½•ï¼ˆæ¨èç”¨äºé•¿è§†é¢‘ï¼‰
    """
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(video_path):
        print(f"é”™è¯¯ï¼šè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return False
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    video_name = get_filename_without_extension(video_path)
    audio_file = f"{video_name}_temp_audio.mp3"  # ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
    text_output_file = f"{video_name}.txt"  # è¾“å‡ºæ–‡æœ¬æ–‡ä»¶
    
    print(f"æ­£åœ¨å¤„ç†è§†é¢‘: {video_path}")
    print(f"è¾“å‡ºæ–‡æœ¬æ–‡ä»¶å°†ä¿å­˜ä¸º: {text_output_file}")
    
    # 1. ä»è§†é¢‘ä¸­æå–éŸ³é¢‘
    if not extract_audio_from_video(video_path, audio_file):
        print("éŸ³é¢‘æå–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†ã€‚")
        return False
    
    # 2. è½¬å½•éŸ³é¢‘
    transcribed_text = None
    
    if use_segmentation:
        print("\n--- æ­£åœ¨ä½¿ç”¨åˆ†æ®µè½¬å½• ---")
        transcribed_text = split_and_transcribe_audio(
            audio_file,
            output_dir=f"{video_name}_audio_chunks",
            min_silence_len=700,
            silence_thresh=-35,
            model_name=model_name
        )
    else:
        print("\n--- æ­£åœ¨ä½¿ç”¨ç›´æ¥è½¬å½• ---")
        transcribed_text = transcribe_audio_with_whisper(audio_file, model_name=model_name)
    
    # 3. ä¿å­˜è½¬å½•ç»“æœ
    if transcribed_text:
        print("\n--- è½¬å½•å®Œæˆ ---")
        print("è½¬å½•ç»“æœé¢„è§ˆ:")
        print(transcribed_text[:200] + "..." if len(transcribed_text) > 200 else transcribed_text)
        
        try:
            with open(text_output_file, "w", encoding="utf-8") as f:
                f.write(transcribed_text)
            print(f"\nè½¬å½•æ–‡æœ¬å·²æˆåŠŸä¿å­˜åˆ°: {text_output_file}")
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    else:
        print("è½¬å½•å¤±è´¥ã€‚")
        return False
    
    # 4. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    try:
        if os.path.exists(audio_file):
            os.remove(audio_file)
            print(f"å·²åˆ é™¤ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶: {audio_file}")
        
        # å¦‚æœä½¿ç”¨äº†åˆ†æ®µè½¬å½•ï¼Œæ¸…ç†åˆ†å—ç›®å½•
        if use_segmentation:
            chunks_dir = f"{video_name}_audio_chunks"
            if os.path.exists(chunks_dir):
                import shutil
                shutil.rmtree(chunks_dir)
                print(f"å·²åˆ é™¤ä¸´æ—¶éŸ³é¢‘åˆ†å—ç›®å½•: {chunks_dir}")
    except Exception as e:
        print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‘ç”Ÿè­¦å‘Š: {e}")
    
    return True

# --- ä¸»è¦æ‰§è¡Œéƒ¨åˆ† ---
if __name__ == "__main__":
    # è·å–ç”¨æˆ·è¾“å…¥çš„è§†é¢‘è·¯å¾„
    video_path = input("è¯·è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„: ").strip().strip('"\'')  # å»é™¤å¯èƒ½çš„å¼•å·
    
    if not video_path:
        print("é”™è¯¯ï¼šæœªæä¾›è§†é¢‘æ–‡ä»¶è·¯å¾„")
        exit(1)
    
    # é€‰æ‹©Whisperæ¨¡å‹
    print("\nå¯ç”¨çš„Whisperæ¨¡å‹:")
    print("1. tiny - æœ€å¿«ï¼Œå‡†ç¡®åº¦æœ€ä½")
    print("2. base - å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®åº¦ï¼ˆæ¨èï¼‰")
    print("3. small - è¾ƒå¥½å‡†ç¡®åº¦")
    print("4. medium - é«˜å‡†ç¡®åº¦")
    print("5. large - æœ€é«˜å‡†ç¡®åº¦ï¼Œéœ€è¦æ›´å¤šèµ„æº")
    
    model_choice = input("\nè¯·é€‰æ‹©æ¨¡å‹ (1-5ï¼Œé»˜è®¤ä¸º2): ").strip()
    
    model_map = {
        "1": "tiny",
        "2": "base", 
        "3": "small",
        "4": "medium",
        "5": "large"
    }
    
    selected_model = model_map.get(model_choice, "base")
    print(f"å·²é€‰æ‹©æ¨¡å‹: {selected_model}")
    
    # é€‰æ‹©è½¬å½•æ–¹å¼
    segmentation_choice = input("\næ˜¯å¦ä½¿ç”¨åˆ†æ®µè½¬å½•ï¼Ÿ(é€‚åˆé•¿è§†é¢‘ï¼Œy/N): ").strip().lower()
    use_segmentation = segmentation_choice in ['y', 'yes', 'æ˜¯']
    
    # å¼€å§‹å¤„ç†
    success = process_video_to_text(
        video_path=video_path,
        model_name=selected_model,
        use_segmentation=use_segmentation
    )
    
    if success:
        print("\nğŸ‰ è§†é¢‘è½¬å½•å®Œæˆï¼")
    else:
        print("\nâŒ è§†é¢‘è½¬å½•å¤±è´¥ã€‚")